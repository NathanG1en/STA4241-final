
---

# STA4241 Final Project

> **Exploring Hierarchical Clustering and DBSCAN w/ a real dataset!**

For STA4241, this repo is meant to run us through two clustering techniques, Hierarchical Clustering and DBSCAN. We explore them through notebooks that are typical for these types of analyses, and we've also developed two webapps that allow us to further play witih and understand the parameters and outputs in the models. 

---

## ğŸš€ Features


### ğŸ““ Jupyter Notebooks

* **From-scratch implementations**
* **Comprehensive analysis** with internal and external validation metrics
---

### ğŸŒ³ Hierarchical Clustering Explorer

* **Interactive dendrograms** with adjustable truncation
* **Multiple linkage methods**: Ward, Single, Complete, Average
* **Real-time clustering** with adjustable cluster count
* **Feature analysis** across clusters with boxplots and heatmaps
* **PCA visualization** with explained variance analysis

### ğŸ¯ DBSCAN Explorer

* **Density-based clustering** with epsilon and min_samples tuning
* **Automatic outlier detection** with noise point visualization
* **Quality metrics**: Silhouette Score, Davies-Bouldin Index, Calinski-Harabasz
* **Multiple distance metrics**: Euclidean, Manhattan, Cosine
* **Interactive parameter exploration** to understand density-based clustering



## ğŸ“¦ Installation

### Prerequisites

* Python 3.11+
* UV package manager (recommended)

### Quick Start

1. **Clone the repository**

```bash
git clone https://github.com/NathanG1en/STA4241-final.git
cd clustering-explorer
```

2. **Install dependencies**

```bash
# Using UV (recommended)
uv sync

# Or using pip
pip install -r requirements.txt
```

3. **Launch the apps**

```bash
# Hierarchical Clustering Explorer
streamlit run HierarchicalClusteringExplorer.py

# DBSCAN Explorer
streamlit run DBSCAN_Hyperparameter_Explorer.py
```

---

## ğŸ“Š Dataset

The project uses the **UCI Wholesale Customers Dataset**, containing annual spending data across six product categories:

* ğŸ¥© Fresh
* ğŸ¥› Milk
* ğŸ›’ Grocery
* ğŸ§Š Frozen
* ğŸ§¼ Detergents_Paper
* ğŸ° Delicassen

**440 samples** Â· **6 features** Â· **Real-world business data**

---
## Each App

### Hierarchical Clustering Explorer

**What you can do:**

* ğŸ›ï¸ Adjust linkage method and cluster count
* ğŸ“ˆ Inspect dendrograms
* ğŸ¨ View clusters in PCA-reduced space
* ğŸ“Š Compare cluster distributions
* ğŸ’¾ Export clustered CSVs

**Key Parameters:**

* Linkage Method
* Number of Clusters
* Dendrogram Truncation

---

### DBSCAN Explorer

**What you can do:**

* Tune epsilon + min_samples
* Visualize cluster density and outliers
* Track metrics in real-time
* Switch between distance metrics
* Save labeled output

**Key Parameters:**

* **Epsilon (eps)**: 0.1â€“3.0
* **Minimum Samples**: 2â€“20
* **Distance Metric**: Euclidean / Manhattan / Cosine

---

## ğŸ“š Notebooks

### `HCA-scratch.ipynb`

**Hierarchical Clustering from Scratch**

Includes:

* Manual distance functions
* All linkage methods
* Side-by-side comparisons

### `HCA-Practical.ipynb`

**HCA Deep Dive**

Includes:

* Internal validation
* External validation (ARI, NMI)
* Visualizations
* Cluster profiling

---

### `DB-scan-Practical.ipynb`

**DBSCAN Deep Dive**

Includes:

* Internal validation
* External validation (ARI, NMI)
* Visualizations
* Cluster profiling

---

## ğŸ¨ Visualizations

The project includes the following graphics:

* PCA scatter plots
* Dendrograms
* Box plots
* Heatmaps
* Explained variance charts'
---

## ğŸ§® Metrics & Evaluation

### Internal Metrics

* **Silhouette Score** â€” higher == better
* **Davies-Bouldin Index** â€” lower == better
* **Calinski-Harabasz Score** â€” higher == better

### External Metrics (in notebooks)
we were given some labels that could possibly serve as ground truths, so we use the following metrics to see hwo they line up:
* **ARI (Adjusted Rand Index)**
* **NMI (Normalized Mutual Information)**

---

## ğŸ› ï¸ What We Used

| Category        | Tools                      |
| --------------- | -------------------------- |
| Core            | Python 3.11, NumPy, Pandas |
| ML              | scikit-learn, SciPy        |
| Visualization   | Matplotlib, Seaborn        |
| Web App         | Streamlit                  |
| Notebooks       | Jupyter                    |
| Package Manager | UV                         |

---

## ğŸ“– Learning

### Understanding the Algorithms

**Hierarchical Clustering**

* Agglomerative tree-building
* Dendrogram-based cluster discovery
* Linkage defines cluster shape

**DBSCAN**

* Density-based clustering
* Automatically identifies noise
* No fixed k required

### Tips

1. Start with defaults
2. Use dendrogram for k
3. Check internal metrics
4. Explore freely

---

## Source of data

* UCI Machine Learning Repository
